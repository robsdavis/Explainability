{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Tutorial 3 - Dynamask\n","\n","In this tutorial we we create a Dynamask explainer object and use it to explain a test record. The explainer is then saved to disk and can be given to someone else to view in the [Interpretability Suite App](https://vanderschaarlab-demo-interpretabi-interpretability-suite-1uteyn.streamlit.app/).\n","\n","We will be explaining the predictions of pytorch convolutional neural net that we have trained and saved separately on an engine noise dataset from IEEE World Congress on Computational Intelligence, 2008. The Interpretability.models module provides a a pytorch model for this that is compatible with trained models `state_dict`s available on the Google Drive link below.\n","\n","### Import the relevant modules"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Standard\n","import os\n","import pathlib\n","\n","# Third Party\n","import numpy as np\n","import torch\n","\n","# Intperpretability\n","from interpretability.models.recurrent_neural_net import ConvNet\n","from interpretability.interpretability_models import dynamask_explainer\n","from interpretability.interpretability_models.utils import io"]},{"cell_type":"markdown","metadata":{},"source":["### Load the data \n","Load the data and split it into the train and the test examples we will explain. This cell will download the data from the `root_url` and save it to a subdirectory in the folder this notebook is being run."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# LOADS\n","def load_forda_data():\n","    def readucr(filename):\n","        data = np.loadtxt(filename, delimiter=\"\\t\")\n","        y = data[:, 0]\n","        x = data[:, 1:]\n","        return x, y.astype(int)\n","\n","    root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n","\n","    x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n","    x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n","\n","    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n","    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n","\n","    idx = np.random.permutation(len(x_train))\n","    x_train = x_train[idx]\n","    y_train = y_train[idx]\n","\n","    y_train[y_train == -1] = 0\n","    y_test[y_test == -1] = 0\n","\n","    return x_train, y_train, x_test, y_test\n","\n","\n","X_train, y_train, X_explain, y_explain = load_forda_data()"]},{"cell_type":"markdown","metadata":{},"source":["### Download the trained model from Google Drive\n","\n","You could train your own model using the ConvNet class and load it here, but we have trained one already.\n","\n","Download the model using this link: https://drive.google.com/file/d/173vniHegUSGmdC6fKCLupynRoxEdz9Ko/view?usp=sharing and save it in a location matching the path `TRAINED_MODEL_STATE_PATH` below (This is the same model as in \"Tutorial_02_implement_simplex_time_series\", so if you have already done so, there is no reason to download it again). The default location is the `\"resources/saved_models\"` folder inside the root Interpretability directory.\n","\n","\n","### Load the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Load the model\n","model = ConvNet()\n","\n","def load_trained_model(model, trained_model_state_path, device='cpu'):\n","    model.load_state_dict(torch.load(trained_model_state_path, map_location=torch.device(device)))\n","    model.eval()\n","    return model\n","\n","DEVICE = \"cpu\"\n","\n","root_path = pathlib.Path.cwd().parents[0]\n","saved_models_path = root_path / \"resources/saved_models\"\n","TRAINED_MODEL_STATE_PATH = saved_models_path / \"model_cv1_2.pth\"\n","model = load_trained_model(model, TRAINED_MODEL_STATE_PATH, device=DEVICE)"]},{"cell_type":"markdown","metadata":{},"source":["### Initialize Dynamask\n","Initialize the explainer object by passing the predictive model, the perturbation method to use, and whether or not to use a `Mask` or `GroupMask`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["available_perturbation_method = [\n","    \"fade_moving_average\",\n","    \"gaussian_blur\",\n","    \"fade_moving_average_window\",\n","    \"fade_moving_average_past_window\",\n","    \"fade_reference\",\n","]\n","\n","my_explainer = dynamask_explainer.DynamaskExplainer(\n","    model, available_perturbation_method[1], group=False\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Fit the explainer\n","\n","Fit the explainer on the training data. This makes explanations of the test data available in the subsequent step."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["available_loss_functions = [\n","    \"cross_entropy\",\n","    \"log_loss\",\n","    \"log_loss_target\",\n","    \"mse\",\n","]\n","\n","explain_id = 1\n","\n","my_explainer.fit(\n","    explain_id,\n","    X_train,\n","    loss_function=available_loss_functions[0],\n","    target=y_train[explain_id],\n","    area_list=np.arange(0.1, 0.5, 0.1),\n",")\n","feature_num, time_step_num = X_train[0].shape"]},{"cell_type":"markdown","metadata":{},"source":["### Get the explanation\n","Get the explanation for the record selected above. We can also choose whether to smooth the mask or not."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\n","    my_explainer.explain(\n","        # ids_feature=[i for i in range(5)],\n","        # ids_time=[i for i in range(5)],\n","        smooth=False,\n","        get_mask_from_group_method=\"extremal\",\n","        extremal_mask_threshold=0.01,\n","    ).feature_importances\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Plot the explanation\n","Plot the feature importance over time. This produces an image as a png."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["my_explainer.summary_plot()"]},{"cell_type":"markdown","metadata":{},"source":["### Save the explainer to file\n","This file can now be uploaded to the [Interpretability Suite App](https://vanderschaarlab-demo-interpretabi-interpretability-suite-1uteyn.streamlit.app/). This provides a non-programtic interface with which to view the various explanations, allowing you to send the explainer to a colleague who is less fluent in python."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["io.save_explainer(\n","    my_explainer, os.path.abspath(\"my_new_forda_conv_dynamask_explainer.p\")\n",")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('interp')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"6fd73f071793638ac14baf0ff0f19e5ab81431475f40d47f0df0002312a62017"}}},"nbformat":4,"nbformat_minor":2}
